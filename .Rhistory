submit()
"I love" %p% "R!"
"I" %p% "love" %p% "R!"
head(flags)
dim(flags)
class(flags)
cls_list <- lapply(flags,class)
cls_list
class(cls_list)
as.character(cls_list)
cls_vect <- sapply(flags,class)
class(cls_vect)
sum(flags$orange)
flag_colors <- flags[, 11:17]
head(flag_colors)
lapply(flag_colors,sum)
sapply(flag_colors,sum)
sapply(flag_colors,mean)
flag_shapes <- flags[, 19:23]
lapply(flag_shapes,range)
shape_mat <- sapply(flag_shapes,range)
shape_mat
class(shape_mat)
unique(c(3,4,5,5,6,6))
unique(c(3, 4, 5, 5, 5, 6, 6))
unique_vals  <- lapply(flags, unique)
unique_vals
lapply(unique_vals,length)
sapply(unique_vals,length)
sapply(flags, unique)
lapply(unique_vals, function(elem) elem[2])
sapply(flags,unique)
vapply(flags,unique,numeric(1))
ok()
sapply(flags,class)
vapply(flags,class,character(1))
?tapply
table(flags$landmass)
table(flags$animate)
tapply(flags$animate,flags$landmass,mean)
tapply(flags$population,flags$red,summary)
tapply(flags$population,flags$landmass,summary)
ls()
class(plants)
dim(plants)
nrow(plants)
ncol(plants)
object.size(plants)
names(plants)
head(plants)
head(plants,10)
tail(plants,15)
summary(plants)
table(plants$Active_Growth_Period)
str(plants)
?sample
sample(1:6,4,replace = TRUE)
sample(1:6,4,replace = TRUE)
sample(1:20,10)
LETTERS
sample(letters)
sample(LETTERS)
prob  <- c(0.3, 0.7)
flips <- sample(c(0,1),100, replace = TRUE, prob = c(0.3,0.7))
flips
sum(flips)
?rbinom
rbinom(1, size = 100, prob =0.7)
flips2  <- rbinom(1, size = 100, prob =0.7)
flips2  <- rbinom(100, size = 1, prob =0.7)
flips2
sum(flips2)
?rnorm
rnorm(10)
rnorm(100,sd = 25)
rnorm(10,sd = 25,mean = 100)
rpois(5,10)
replacate(100,rpois(5,10))
replicate(100,rpois(5,10))
my_pois <- replicate(100,rpois(5,10))
my_pois
colMeans(my_pois)
cm <- colMeans(my_pois)
hist(cm)
d1 <- Sys.Date()
class(d1)
unclass(d1)
d1
d2 <- as.Date("1969-01-01")
unclass(d2)
t1 <- Sys.time()
t1
class(t1)
unclass(t1)
t2 <- as.POSIXlt(Sys.time())
t2
class(t2)
t2
unclass(t2)
str(unclass(t2))
t2$min
weekdays(t1)
weekdays(d1)
months(t1)
quarters(t2)
t3 <- "October 17, 1986 08:24"
t4 <- strptime(t3, "%B %d, %Y %H:%M")
t4
class(t4)
Sys.time() > t1
Sys.time() - t1
difftime(Sys.time(),t1, units = 'days')
data(cars)
?cars
head(cars)
plot(cars)
?plot
plot(x=cars$speed,y=cars$dist)
plot(dist ~ speed, cars)
plot(y=cars$speed,x=cars$dist)
plot(Speed,cars)
plot("Speed",cars)
plot(y=cars$speed,x=cars$dist,"Speed")
plot(y=cars$speed,x=cars$dist,xlab="Speed")
plot(y=cars$dist,x=cars$speed,xlab="Speed")
plot(y=cars$dist,x=cars$speed,xlab="Speed",ylab = "Stopping Distance")
plot(y=cars$dist,x=cars$speed,ylab = "Stopping Distance")
plot(y=cars$dist,x=cars$speed,xlab="Speed",ylab = "Stopping Distance")
plot(y=cars$dist,x=cars$speed,xlab="Speed",ylab = "Stopping Distance", main = "My Plot")
plot(cars, main = "My Plot")
plot(cars, main = "My Plot Subtitle")
plot(cars, sub = "My Plot Subtitle")
plot(cars, col=2)
plot(cars, xlim=c(10,15))
plot(cars, pch=2)
load(mtcars)
data.frame(mtcars)
data(mtcars)
?boxplot
box(mpg ~ cyl, data=mtcars)
boxplot(mpg ~ cyl, data=mtcars)
hist(mtcars$mpg)
fit <- lm(mpg~cyl,mtcars)
fit
fit <- lm(cyl~cyl,mtcars)
fit <- lm(cyl~mpg,mtcars)
fit
coef(fit)
library(UsingR)data(diamond)library(ggplot2)
library(UsingR)
data(diamond)
library(ggplot2)
g <- ggplot(diamond,aes(x=carat,y=price))
g <- ggplot(diamond,aes(x=carat,y=price))
g <- g + geom_point(size=7,colour="balck",alpha=0.5)
g <- g + geom_smooth(method="lm",colour="balck")
g
g <- ggplot(diamond,aes(x=carat,y=price))
g <- g + geom_point(size=7,colour="black",alpha=0.5)
g <- g + geom_smooth(method="lm",colour="black")
g
g <- g + geom_point(size=7,colour="blue",alpha=0.7)
g <- g + geom_smooth(method="lm",colour="black")
g
g <- ggplot(diamond,aes(x=carat,y=price))
g <- g + geom_point(size=7,colour="black",alpha=0.5)
g <- g + geom_point(size=5,colour="blue",alpha=0.2)
g <- g + geom_smooth(method="lm",colour="black")
g
library(ggplot2)
g <- ggplot(mtcars,aes(x=mpg,y=cyl))
g <- g + geom_point(size=7,colour="black",alpha=0.5)
g <- g + geom_point(size=5,colour="blue",alpha=0.2)
g <- g + geom_smooth(method="lm", colour="black")
g
fit <- lm(cyl~mpg,mtcars)
coef(fit)
?I
I(mpg-mean(mpg))
fit <- lm(cyl~I(mpg-mean(mpg)),mtcars)
coef(fit)
fit <- lm(cyl~mpg,mtcars)
coef(fit)
coef(fit)[1] + coef(fit)[2] * newx
newx <- c(0.16, 0.27, 0.34)
coef(fit)[1] + coef(fit)[2] * newx
fit <- lm(cyl~mpg,mtcars)
coef(fit)
newx <- c(0.16, 0.27, 0.34)
coef(fit)[1] + coef(fit)[2] * newx
newx <- c(4,8)
coef(fit)[1] + coef(fit)[2] * newx
10.250623  - 9.240564
fit <- lm(price ~ carat, data = diamond)
coef(fit)
newx <- c(0.16, 0.27, 0.34)
coef(fit)[1] + coef(fit)[2] * newx
predict(fit, newdata = data.frame(carat = newx))
fit <- lm(cyl~mpg,mtcars)
coef(fit)
newx <- c(4,8)
coef(fit)[1] + coef(fit)[2] * newx
predict(fit, newdata = data.frame(carat = newx))
fit <- lm(cyl~I(mpg-mean(mpg)),mtcars)
coef(fit)
fit <- lm(cyl~mpg,mtcars)
coef(fit)
newx <- c(4,8)
coef(fit)[1] + coef(fit)[2] * newx
predict(fit, newdata = data.frame(carat = newx))
predict(fit, newdata = data.frame(cyl = newx))
fit <- lm(mpg~I(cyl-mean(cyl)),mtcars)
coef(fit)
fit <- lm(mpg~cyl,mtcars)
coef(fit)
newx <- c(4,8)
coef(fit)[1] + coef(fit)[2] * newx
predict(fit, newdata = data.frame(cyl = newx))
summary(fit)$coef
fit <- lm(mpg~cyl+wt,mtcars)
coef(fit)
summary(fit)$coef
summary(fit)$coef[3,1]
fit <- lm(mpg~cyl+wt,mtcars)
summary(fit)$coef[3,1]
mtcars$cyl <- factor(mtcars$cyl)
fit <- lm(mpg~cyl+wt,mtcars)
coef(fit)
summary(fit)$coef[3,1]
coef(fit)[1] + coef(fit)[2] * newx
predict(fit, newdata = data.frame(cyl = newx))
newx <- c(4,8)
coef(fit)[1] + coef(fit)[2] * newx
predict(fit, newdata = data.frame(cyl = newx))
summary(fit)$coef[3,1]
coef(fit)
summary(fit)$coef[3,1]
mtcars$cyl <- factor(mtcars$cyl)
fit <- lm(mpg ~ cyl, mtcars)
coef(fit)
summary(fit)$coef[3,1]
coef(fit)
mtcars$cyl <- factor(mtcars$cyl)
mtcars$cyl <- factor(mtcars$cyl)
fit_with_i <- lm(mpg ~ cyl + wt + cyl:wt)
fit_with_i <- lm(mpg ~ cyl + wt + cyl:wt,mtcars)
fit_with_i
summary(fit_with_i)
summary(fit_with_i)$coef
fit_without_i <- lm(mpg ~ cyl + wt, mtcars)
summary(fit_without_i)$coef
install.packages("lmtest")
library(lmtest)
summary(fit_with_i)$adj.r.squared
summary(fit_without_i)$adj.r.squared
lrtest(fit_interaction, fit_non_interaction)
lrtest(fit_with_i, fit_without_i)
lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
summary(fit)$coef
fit <- lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
summary(fit)$coef
fit <- lm(mpg~cyl+wt,mtcars)
coef(fit)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
df <- data.frame(x=x,y=y)
df
g <- ggplot(df,aes(x=x,y=y))
g <- geom_point(size=7,colour="balck",alpha=0.5)
g <- geom_point(size=7,colour="black",alpha=0.5)
g <- ggplot(df,aes(x=x,y=y))
g <- geom_point(size=7,colour="black",alpha=0.5)
g <- geom_point(size=5,colour="blue",alpha=0.2)
g <- geom_smooth(method="lm",colour="black")
g
g <- ggplot(df,aes(x=x,y=y))
g <- g + geom_point(size=7,colour="black",alpha=0.5)
g <- g +geom_point(size=5,colour="blue",alpha=0.2)
g <- g +geom_smooth(method="lm",colour="black")
g
fit <- lm(x ~ y, df)
summary(fit)
summary(fit)$adj.r.squared
max(hatvalues(fit))
fit <- lm(y ~ x, df)
summary(fit)$adj.r.squared
max(hatvalues(fit))
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y ~ x)
influence.measures(fit)$infmat[5, 'dfb.x']
influence.measures(fit)
influence.measures(fit)$infmat[5, 'dfb.x']
mtcars$cyl <- factor(mtcars$cyl)
fit <- lm(mpg~cyl+wt,mtcars)
coef(fit)
summary(fit)$coef[3,1]
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
df <- data.frame(x=x,y=y)
g <- ggplot(df,aes(x=x,y=y))
g <- g + geom_point(size=7,colour="black",alpha=0.5)
g <- g +geom_point(size=5,colour="blue",alpha=0.2)
g <- g +geom_smooth(method="lm",colour="black")
g
fit <- lm(y ~ x, df)
summary(fit)$adj.r.squared
max(hatvalues(fit))
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y ~ x)
influence.measures(fit)$infmat[5, 'dfb.x']
library(lmtest)
mtcars$cyl <- factor(mtcars$cyl)
fit_with_i <- lm(mpg ~ cyl + wt + cyl:wt,mtcars)
fit_without_i <- lm(mpg ~ cyl + wt, mtcars)
summary(fit_with_i)$adj.r.squared
summary(fit_without_i)$adj.r.squared
lrtest(fit_with_i, fit_without_i)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
df <- data.frame(x=x,y=y)
g <- ggplot(df,aes(x=x,y=y))
g <- g + geom_point(size=7,colour="black",alpha=0.5)
g <- g +geom_point(size=5,colour="blue",alpha=0.2)
g <- g +geom_smooth(method="lm",colour="black")
g
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
df <- data.frame(x=x,y=y)
g <- ggplot(df,aes(x=x,y=y))
data("iris")
i <- data("iris")
i
iris
iris
install.packages("kknn")
library("kknn")
rm(list = ls(all=T))
muestra <- sample(1:150,50)
muestra
ttesting <- iris[muestr,]
ttesting <- iris[muestra,]
ttesting <- iris[muestra,]
taprendizaje <- iris[-muestra,]
View(ttesting)
modelo <- train.kknn(tipo~.,data = taprendizaje,kmax = 9)
taprendizaje
modelo <- train.kknn(Species~.,data = taprendizaje,kmax = 9)
modelo
prediccion <- predict(modelo,ttesting[,-5])
prediccion
mc <- table(ttesting[,5],prediccion)
mc
acierto <- (sum(diag(mc)))/sum(mc)
acierto
error <- 1 - acierto
error
iris
rm(list = ls(all=T))
muestra <- sample(1:150,50)
ttesting <- iris[muestra,]
taprendizaje <- iris[-muestra,]
modelo <- train.kknn(Species~.,data = taprendizaje,kmax = 9)
#k podria ser la raiz cuadrada de n (impares son mejores)
modelo
modelo <- train.kknn(Species~.,data = taprendizaje,kmax = 13)
#k podria ser la raiz cuadrada de n (impares son mejores)
modelo
##Funcion Polimorfica
prediccion <- predict(modelo,ttesting[,-5])
prediccion
#Matriz Confusion
mc <- table(ttesting[,5],prediccion)
mc
acierto <- (sum(diag(mc)))/sum(mc)
acierto
error <- 1 - acierto
error
library(FactoMineR)
res  <- PCA(mtcars[,1:8],scale.unit = TRUE, ncp=5, graph = FALSE)
plot(res, axes = c(1,2), choix = "ind", col.ind = "red", new.plot = FALSE)
plot(res, axes = c(1,2), choix = "var", col.var = "blue", new.plot = FALSE)
cos2.ind <- (res$ind$cos2[,1]+res$ind$cos2[,2])*100
cos2.ind
cos2.var <- (res$var$cos2[,1]+res$var$cos2[,2])*100
cos2.var
res.hcpc <- HCPC(res,nb.clust = -1,consol = TRUE,min = 3,max = 3,graph = FALSE)
plot.HCPC(res.hcpc,choice = "bar")
plot.HCPC(res.hcpc,choice = "map")
manipulate(plot.HCPC(res.hcpc,choice = "3D.map",angle=myangle), myangle = slider(0,180, step=5))
library(UsingR)
library(manipulate)
manipulate(plot.HCPC(res.hcpc,choice = "3D.map",angle=myangle), myangle = slider(0,180, step=5))
centers <- centers.hclust(mtcars[,1:8],model,nclust = 3,use.median = FALSE)
library(FactoMineR)
kgroups <- kmeans(mtcars[,1:8],3,iter.max = 100)
kgroups$cluster
barplot(t(kgroups$centers),beside = T)
library(caret)
library(kernlab)
library(ISLR)
library(ggplot2)
library(Hmisc)
library(gridExtra)
data("spam")
data("Wage")
inTrain <- createDataPartition(y=spam$type,p=0.75,list = FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
q1 <- qplot(capitalAve,data = training) + geom_histogram()
q2 <- qplot(capitalAve,data = training,geom = "density")
grid.arrange(q1,q2,ncol=2)
mean(training$capitalAve)
sd(training$capitalAve)
trainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve-mean(trainCapAve))/sd(trainCapAve)
mean(trainCapAveS)
sd(trainCapAveS)
testCapAve <- testing$capitalAve
testCapAveS <- (testCapAve-mean(trainCapAve))/sd(trainCapAve)
mean(testCapAveS)
sd(testCapAveS)
preObj <- preProcess(training[,-58],method = c("center","scale"))
trainCapAveS <- predict(preObj,training[,-58])$capitalAve
mean(trainCapAveS)
sd(trainCapAveS)
preObj <- preProcess(training[,-58],method = c("center","scale"))
trainCapAveS <- predict(preObj,training[,-58])$capitalAve
mean(trainCapAveS)
sd(trainCapAveS)
testCapAveS <- predict(preObj,testing[,-58])$capitalAve
mean(testCapAveS)
sd(trainCapAveS)
sd(testCapAveS)
hist(trainCapAveS)
preObj <- preProcess(training[,-58],method = c("BoxCox"))
trainCapAveS <- predict(preObj,training[,-58])$capitalAve
mean(trainCapAveS)
sd(trainCapAveS)
hist(trainCapAveS)
qqnorm(trainCapAveS)
preObj <- preProcess(training[,-58],method = c("center","scale"))
trainCapAveS <- predict(preObj,training[,-58])$capitalAve
hist(trainCapAveS)
qqnorm(trainCapAveS)
preObj <- preProcess(training[,-58],method = c("BoxCox"))
trainCapAveS <- predict(preObj,training[,-58])$capitalAve
mean(trainCapAveS)
sd(trainCapAveS)
hist(trainCapAveS)
qqnorm(trainCapAveS)
library(UsingR)
library(manipulate)
library(FactoMineR)
res  <- PCA(mtcars[,1:8],scale.unit = TRUE, ncp=5, graph = FALSE)
plot(res, axes = c(1,2), choix = "ind", col.ind = "red", new.plot = FALSE)
plot(res, axes = c(1,2), choix = "var", col.var = "blue", new.plot = FALSE)
cos2.ind <- (res$ind$cos2[,1]+res$ind$cos2[,2])*100
cos2.ind
plot(res, axes = c(1,2), choix = "ind", col.ind = "red", new.plot = FALSE, select = "cos2 0.5")
plot(res, axes = c(1,2), choix = "var", col.var = "blue", new.plot = FALSE, select = "cos2 0.4")
cos2.var <- (res$var$cos2[,1]+res$var$cos2[,2])*100
cos2.var
res.hcpc <- HCPC(res,nb.clust = -1,consol = TRUE,min = 3,max = 3,graph = FALSE)
plot.HCPC(res.hcpc,choice = "bar")
plot.HCPC(res.hcpc,choice = "map")
manipulate(plot.HCPC(res.hcpc,choice = "3D.map",angle=myangle), myangle = slider(0,180, step=5))
dim(training)
dim(training)[1]
rbinom(dim(training)[1])
rbinom(dim(training)[1],size = 1)
rbinom(dim(training)[1],size = 1,prob = 0.05)
rbinom(dim(training)[1],size = 1,prob = 0.05) == 1
selectNA <- rbinom(dim(training)[1],size = 1,prob = 0.05) == 1
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1],size = 1,prob = 0.05) == 1
training$capAve[selectNA] <- NA
preObj <- preProcess(training[,-58],method = c("knnImpute"))
capAve <- predict(preObj,training[,-58])$capAve
preObj <- preProcess(training[,-58],method = c("knnImpute"))
capAve <- predict(preObj,training[,-58])$capAve
install.packages("RANN")
library(RANN)
capAve <- predict(preObj,training[,-58])$capAve
mean(capAve)
sd(capAve)
hist(capAve)
qqnorm(capAve)
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)
setwd("Google Drive/Coursera/Data Science Specialization/Machine Learning/Week Three/machinelearning/")
dataTraining <- read.csv("../pml-training.csv",header = T)
dataTesting <- read.csv("../pml-testing.csv",header = T)
inTrain <- createDataPartition(y=dataTraining$classe,p=0.6,list = FALSE)
training <- dataTraining[inTrain,]
testing <- dataTraining[-inTrain,]
varnumerics <- c()
other <- c()
for(name in colnames(training)){
myclass <- class(training[,c(name)])
if( myclass == "numeric" | myclass == "integer" ){
varnumerics <- c(varnumerics,name)
}else{
other <- c(other,name)
}
}
M <- abs(cor(training[,varnumerics]))
diag(M) <- 0
r <- which(M > 0.8, arr.ind = T)
nt <- training[,c(rownames(r),other)]
ntest <- testing[,c(rownames(r),other)]
